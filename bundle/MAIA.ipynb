{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865749bb217bdd8b",
   "metadata": {},
   "source": [
    "# MAIA\n",
    "\n",
    "This tutorial has the purpose to showcase all the major applications and functionalities available in MAIA. In detail, we will cover all the essential Medical AI lifecycle stages, to provide a comprehensive overview of the platform.\n",
    "\n",
    "The tutorial is based on two different datasets:\n",
    "- [**Decathlon Spleen**](http://medicaldecathlon.com/): a dataset of 3D spleen CT scans from the Medical Segmentation Decathlon challenge. This NIFTI dataset is used to demonstrate the model preprocessing, training and evaluation functionalities in MAIA.\n",
    "- [**CT Lymph Nodes**](https://www.cancerimagingarchive.net/collection/ct-lymph-nodes/) from TCIA: a dataset of 3D lymph node CT scans from The Cancer Imaging Archive. This DICOM dataset is used to demonstrate the data management functionalities in MAIA, including DICOM upload,visualization, annotation and AI model inference (including Active Learning with MONAI Label).\n",
    "\n",
    "The tutorial will cover all the necessary steps to download the Decathlon Spleen dataset, preprocess it, train a [ResEnc nnU-Net](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md) model as a MONAI Bundle, evaluate the model, and finally deploy it for inference on the CT Lymph Nodes dataset. The tutorial will also cover the necessary steps to upload the CT Lymph Nodes dataset, visualize it, annotate it, and perform AI model inference on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59583f0bd43246ad",
   "metadata": {},
   "source": [
    "## Setup environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5447525-e0e3-4298-87d8-2efe012d931a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install odict plotly dtale \"monai[nibabel, skimage, scipy, pillow, tensorboard, gdown, ignite, torchvision, itk, tqdm, pandas, mlflow, matplotlib, pydicom]\" \"pydicom==2.4.4\" nnunetv2==2.5.1 fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fbf6aeed0a7dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'monai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapps\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecathlonDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'monai'"
     ]
    }
   ],
   "source": [
    "from monai.apps import DecathlonDataset\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "import dtale\n",
    "import dtale.app as dtale_app\n",
    "\n",
    "from monai.bundle import ConfigParser\n",
    "\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types.schema import Schema, TensorSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c47266462d9e08",
   "metadata": {},
   "source": [
    "## Download the Decathlon Spleen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d20edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66acadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6902b6e42cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecathlonDataset(root_dir=root_dir,\"Task09_Spleen\",\"training\",download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e7d474b6d8ed0",
   "metadata": {},
   "source": [
    "## Visualize Spleen CT and SEG in 3D Slicer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24e17f-4757-45ab-9d17-0de5a8ca23ad",
   "metadata": {},
   "source": [
    "You can navigate to [Remote Desktop](/user/{USERNAME}/proxy/80/desktop/{USERNAME}) to interact with the Remote Desktop environment and 3D Slicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de310610-a2b6-47d1-a4c3-0d25ce066128",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer_executable = \"/home/maia-user/Documents/Slicer-5.7.0-2024-08-05-linux-amd64/Slicer\"\n",
    "\n",
    "case_id = \"spleen_2\"\n",
    "\n",
    "ct_volume = Path(root_dir).joinpath(\"Task09_Spleen/imagesTr\",f\"{case_id}.nii.gz\")\n",
    "\n",
    "seg_volume = Path(root_dir).joinpath(\"Task09_Spleen/labelsTr\",f\"{case_id}.nii.gz\")\n",
    "\n",
    "subprocess.run([\n",
    "    slicer_executable,\n",
    "    \"--python-code\", f\"slicer.util.loadVolume('{ct_volume}'); seg=slicer.util.loadSegmentation('{seg_volume}'); seg.CreateClosedSurfaceRepresentation()\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28763d4-f46e-4274-9f0d-ad0314cd39b9",
   "metadata": {},
   "source": [
    "## nnUNet Training\n",
    "\n",
    "To run the nnUNet Training, follow the Tutorial [nnUnet Bundle Training](06_nnunet_monai_bundle.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7642f",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37887f39c3655ad5",
   "metadata": {},
   "source": [
    "To visualize in the 3D Slicer the fold-0 validation predictions, together with the ground truth and the CT volume, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(root_dir).joinpath(\"nnUNet/nnUNet_raw_data_base/Dataset009_Task09_Spleen/datalist.json\")) as f:\n",
    "    datalist = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d0dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_0_datalist_cases = [case for case in datalist['training'] if case['fold'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9843766b-ae37-4e53-aade-b3d2da5755d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch to module:  \"Welcome\"\n",
      "\"Volume\" Reader has successfully read the file \"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/Task09_Spleen/imagesTr/spleen_26.nii.gz\" \"[0.57s]\"\n",
      "ReferenceImageExtentOffset attribute was not found in NRRD segmentation file. Assume no offset.\n",
      "\n",
      "\n",
      "\"Segmentation\" Reader has successfully read the file \"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/nnUNet/nnUNet_trained_models/Dataset009_Task09_Spleen/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/validation/case_0.nii.gz\" \"[0.21s]\"\n",
      "ReferenceImageExtentOffset attribute was not found in NRRD segmentation file. Assume no offset.\n",
      "\n",
      "\n",
      "\"Segmentation\" Reader has successfully read the file \"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/Task09_Spleen/labelsTr/spleen_26.nii.gz\" \"[0.20s]\"\n",
      "Switch to module:  \"Data\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m seg_volume \u001b[38;5;241m=\u001b[39m Path(root_dir)\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask09_Spleen/labelsTr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m pred_volume \u001b[38;5;241m=\u001b[39m Path(root_dir)\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnnUNet/nnUNet_trained_models/Dataset009_Task09_Spleen/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,new_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslicer_executable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--python-code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslicer.util.loadVolume(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mct_volume\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m); pred=slicer.util.loadSegmentation(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpred_volume\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m); seg=slicer.util.loadSegmentation(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseg_volume\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m); seg.CreateClosedSurfaceRepresentation()\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/MONAI/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/.conda/envs/MONAI/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/MONAI/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/MONAI/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/.conda/envs/MONAI/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "slicer_executable = \"/home/maia-user/Documents/Slicer-5.7.0-2024-08-05-linux-amd64/Slicer\"\n",
    "\n",
    "case =fold_0_datalist_cases[0]\n",
    "\n",
    "case_id = case['image'].split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "new_name = case[\"new_name\"]\n",
    "\n",
    "\n",
    "ct_volume = Path(root_dir).joinpath(\"Task09_Spleen/imagesTr\",f\"{case_id}.nii.gz\")\n",
    "\n",
    "seg_volume = Path(root_dir).joinpath(\"Task09_Spleen/labelsTr\",f\"{case_id}.nii.gz\")\n",
    "\n",
    "pred_volume = Path(root_dir).joinpath(\"nnUNet/nnUNet_trained_models/Dataset009_Task09_Spleen/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/validation\",new_name+\".nii.gz\")\n",
    "\n",
    "subprocess.run([\n",
    "    slicer_executable,\n",
    "    \"--python-code\", f\"slicer.util.loadVolume('{ct_volume}'); pred=slicer.util.loadSegmentation('{pred_volume}'); seg=slicer.util.loadSegmentation('{seg_volume}'); seg.CreateClosedSurfaceRepresentation()\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c49bb5aaa8f16",
   "metadata": {},
   "source": [
    "### Validation Metrics\n",
    "\n",
    "The validation metrics are stored in a JSON file named `summary.json` in the validation folder. We can load the file and visualize the metrics with [DTale](https://github.com/man-group/dtale) using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2eb82a-331f-4616-bdd9-0030f960cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file = Path(root_dir).joinpath(\"nnUNet/nnUNet_trained_models/Dataset009_Task09_Spleen/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/validation\",\"summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf643ae-bf75-4d18-81c0-e7d6e968d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(summary_file) as f:\n",
    "    summary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85500327",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"label_dict\":{\n",
    "        \"Background\": 0,\n",
    "        \"Spleen\": 1\n",
    "    },\n",
    "    \"label_suffix\": \".nii.gz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feda6346-f6a1-4282-8dc6-ba6f5e56a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "label_to_name = {v: k for k, v in config_dict[\"label_dict\"].items()}\n",
    "\n",
    "for case in summary['metric_per_case']:\n",
    "    for label_id in case['metrics']:\n",
    "        for metric in case['metrics'][label_id]:\n",
    "           \n",
    "            df.append({\n",
    "                \"Case\": Path(case['reference_file']).name[:-len(config_dict[\"label_suffix\"])],\n",
    "                \"Label\": label_to_name[int(label_id)],\n",
    "                \"Metric\": metric,\n",
    "                \"Value\": case['metrics'][label_id][metric]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e77bb2-6f72-4b44-acb3-c10685baf887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444b24cf-182d-4183-8fb3-bf9dda62b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale_app.JUPYTER_SERVER_PROXY = True\n",
    "\n",
    "d = dtale.show(df,host=\"0.0.0.0\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa3f1c1-647e-47a2-ace4-7c5ad8dce784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import os\n",
    "\n",
    "\n",
    "DTALE_URL = d._main_url\n",
    "@register_cell_magic\n",
    "def markdown(line, cell):\n",
    "    return Markdown(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbdefb9b-9359-42e8-aa45-830e41aced6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "[DTale](/user/simben@kth.se/proxy/40000/dtale/main/1)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%markdown\n",
    "\n",
    "[DTale]({DTALE_URL})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e1768177a6b7c",
   "metadata": {},
   "source": [
    "The DTale charts can be recreated and visualized in the notebook using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660e7045-c349-40b4-a8be-71fd544a9690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.0.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):\n",
    "\tdf = df.to_frame(index=False)\n",
    "\n",
    "# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required\n",
    "df = df.reset_index().drop('index', axis=1, errors='ignore')\n",
    "df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers\n",
    "\n",
    "df = df.query(\"\"\"`Metric` == 'Dice'\"\"\")\n",
    "\n",
    "chart_data = pd.concat([\n",
    "\tdf['Case'],\n",
    "\tdf['Value'],\n",
    "], axis=1)\n",
    "chart_data = chart_data.sort_values(['Case'])\n",
    "chart_data = chart_data.rename(columns={'Case': 'x'})\n",
    "chart_data = chart_data.dropna()\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "charts = []\n",
    "charts.append(go.Bar(\n",
    "\tx=chart_data['x'],\n",
    "\ty=chart_data['Value']\n",
    "))\n",
    "figure = go.Figure(data=charts, layout=go.Layout({\n",
    "    'barmode': 'group',\n",
    "    'legend': {'orientation': 'h', 'y': -0.3},\n",
    "    'title': {'text': 'Validation Fold 0, Dice score'},\n",
    "    'xaxis': {'title': {'text': 'Case'}},\n",
    "    'yaxis': {'title': {'text': 'Dice'}, 'type': 'linear'}\n",
    "}))\n",
    "\n",
    "# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:\n",
    "#\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "#\n",
    "init_notebook_mode(connected=True)\n",
    "for chart in charts:\n",
    "    chart.pop('id', None) # for some reason iplot does not like 'id'\n",
    "#iplot(figure)\n",
    "\n",
    "figure.write_html(\"Fold_0_Val_Dice.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c328ca1d-1e92-4b95-8a8b-0c75d8ae76dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dice</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.970359</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>0.944765</td>\n",
       "      <td>0.965239</td>\n",
       "      <td>0.969701</td>\n",
       "      <td>0.978839</td>\n",
       "      <td>0.985021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Value                                                              \\\n",
       "       count      mean       std       min       25%       50%       75%   \n",
       "Metric                                                                     \n",
       "Dice     9.0  0.970359  0.012429  0.944765  0.965239  0.969701  0.978839   \n",
       "\n",
       "                  \n",
       "             max  \n",
       "Metric            \n",
       "Dice    0.985021  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"Metric\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5afd83c8-edb8-49ae-a9c7-03c231bd3363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9703585638730954"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"Metric\"]).describe()['Value']['mean'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c055b67677ddff",
   "metadata": {},
   "source": [
    "Finally, we can upload the validation metrics and the plots to MLFlow using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7a4a7-162b-4a47-b1cf-e51b9101b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "mlflow.set_experiment(\"Task09_Spleen\")\n",
    "\n",
    "with mlflow.start_run(run_id=\"1b31a872e1b644cb9787cf91b60be449\"):\n",
    "    mean_dice = df.groupby([\"Metric\"]).describe()['Value']['mean'].values[0]\n",
    "    \n",
    "    mlflow.log_metric(\"Val_Dice_Fold_0\", mean_dice)\n",
    "    mlflow.log_artifact(\"Fold_0_Val_Dice.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdba2c483d1906f",
   "metadata": {},
   "source": [
    "In the final step of the validation phase, we export the trained model, saving the nnUNet Bundle as a zip file (`Task09_Spleen_nnUNet.zip`). The zip file contains the model, the configuration files, and the environment files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b53c9cfa-8eba-46d5-bb48-8c82e793f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 3d_fullres\n",
      "Exporting fold_0\n",
      "No ensemble directory found for task 009\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export nnUNet_raw=\"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/nnUNet/nnUNet_raw_data_base\"\n",
    "export nnUNet_preprocessed=\"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/nnUNet/nnUNet_preprocessed\"\n",
    "export nnUNet_results=\"/home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/nnUNet/nnUNet_trained_models\"\n",
    "\n",
    "touch /home/maia-user/Documents/GitHub/tutorials/bundle/MONAI/Data/nnUNet/nnUNet_trained_models/Dataset009_Task09_Spleen/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/progress.png\n",
    "\n",
    "nnUNetv2_export_model_to_zip -d 009 -c 3d_fullres -f 0 -o Task09_Spleen_nnUNet.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610c3f7b3b01f30",
   "metadata": {},
   "source": [
    "## Package MONAI Bundle\n",
    "\n",
    "After completing the training and validation of the nnUNet model, we can package the model as a MONAI Bundle, to be used for inference (i.e. Active Learning with MONAI Label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4828ab-9650-4c36-b797-ac6d94b30feb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export BUNDLE_ROOT=nnUNetBundle\n",
    "export PYTHONPATH=$PYTHONPATH:$BUNDLE_ROOT\n",
    "\n",
    "python -m monai.bundle run \\\n",
    "    --config-file $BUNDLE_ROOT/configs/inference.yaml \\\n",
    "    --bundle-root $BUNDLE_ROOT \\\n",
    "    --data-dir $BUNDLE_ROOT/test_input \\\n",
    "    --output-dir $BUNDLE_ROOT/test_output \\\n",
    "    --logging-file $BUNDLE_ROOT/configs/logging.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218be4f305153db",
   "metadata": {},
   "source": [
    "To visualize the prediction in 3D Slicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2a5db0-1d8b-4130-a4a5-3e947b342da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch to module:  \"Welcome\"\n",
      "\"Volume\" Reader has successfully read the file \"/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/test_input/spleen_1/spleen_1.nii.gz\" \"[0.32s]\"\n",
      "ReferenceImageExtentOffset attribute was not found in NRRD segmentation file. Assume no offset.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: In vtkMRMLSegmentationStorageNode.cxx, line 702\n",
      "vtkMRMLSegmentationStorageNode (0xffd0160): vtkMRMLSegmentationStorageNode::ReadBinaryLabelmapRepresentation: Segmentation is a floating point scalar type and will be cast to an integer type by truncation (rounding towards 0).\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Segmentation\" Reader has successfully read the file \"/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/test_output/spleen_1/spleen_1_prediction.nii.gz\" \"[0.21s]\"\n",
      "Switch to module:  \"\"\n",
      "Switch to module:  \"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/maia-user/Documents/Slicer-5.7.0-2024-08-05-linux-amd64/Slicer', '--python-code', \"slicer.util.loadVolume('/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/test_input/spleen_1/spleen_1.nii.gz'); pred=slicer.util.loadSegmentation('/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/test_output/spleen_1/spleen_1_prediction.nii.gz'); pred.CreateClosedSurfaceRepresentation()\"], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "slicer_executable = \"/home/maia-user/Documents/Slicer-5.7.0-2024-08-05-linux-amd64/Slicer\"\n",
    "\n",
    "bundle_root = \"/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle\"\n",
    "data_dir = os.path.join(bundle_root, \"test_input\")\n",
    "ct_volume = os.path.join(data_dir, \"spleen_1\",\"spleen_1.nii.gz\")\n",
    "\n",
    "pred_volume = os.path.join(bundle_root, \"test_output\", \"spleen_1\", \"spleen_1_prediction.nii.gz\")\n",
    "\n",
    "subprocess.run([\n",
    "    slicer_executable,\n",
    "    \"--python-code\", f\"slicer.util.loadVolume('{ct_volume}'); pred=slicer.util.loadSegmentation('{pred_volume}'); pred.CreateClosedSurfaceRepresentation()\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f21258d74bfa7",
   "metadata": {},
   "source": [
    "We finally export the python environment and the requirements for the MONAI Bundle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f844b1f8-7960-4cf9-97e3-884db90e18d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -nunetv2 (/home/maia-user/.conda/envs/MONAI/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "conda env export -n MONAI > nnUNetBundle/environment.yml\n",
    "python -m pip freeze > nnUNetBundle/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b38cca-e5c1-4f56-bd44-e03097653e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: nnUNetBundle/ (stored 0%)\n",
      "  adding: nnUNetBundle/test_output/ (stored 0%)\n",
      "  adding: nnUNetBundle/test_output/spleen_1/ (stored 0%)\n",
      "  adding: nnUNetBundle/test_output/spleen_1/spleen_1_prediction.nii.gz (deflated 87%)\n",
      "  adding: nnUNetBundle/docs/ (stored 0%)\n",
      "  adding: nnUNetBundle/docs/README.md (deflated 49%)\n",
      "  adding: nnUNetBundle/LICENSE (stored 0%)\n",
      "  adding: nnUNetBundle/logs/ (stored 0%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738945616.jupyter-simben-40kth-2ese.531043.0 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738936466.jupyter-simben-40kth-2ese.372773.0 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1739311133.jupyter-simben-40kth-2ese.689007.0 (deflated 70%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738945553.jupyter-simben-40kth-2ese.528235.1 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738936984.jupyter-simben-40kth-2ese.423421.1 (deflated 56%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1739124151.jupyter-simben-40kth-2ese.1617072.1 (deflated 67%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1739174355.jupyter-simben-40kth-2ese.1839409.0 (deflated 70%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738941994.jupyter-simben-40kth-2ese.502814.1 (deflated 66%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738936829.jupyter-simben-40kth-2ese.407258.0 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738945674.jupyter-simben-40kth-2ese.533783.0 (deflated 68%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738945553.jupyter-simben-40kth-2ese.528235.0 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1739311133.jupyter-simben-40kth-2ese.689007.1 (deflated 67%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738936711.jupyter-simben-40kth-2ese.394459.0 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738945616.jupyter-simben-40kth-2ese.531043.1 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738941994.jupyter-simben-40kth-2ese.502814.0 (deflated 58%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738945674.jupyter-simben-40kth-2ese.533783.1 (deflated 66%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738936829.jupyter-simben-40kth-2ese.407258.1 (deflated 9%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1739174355.jupyter-simben-40kth-2ese.1839409.1 (deflated 67%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1739124151.jupyter-simben-40kth-2ese.1617072.0 (deflated 61%)\n",
      "  adding: nnUNetBundle/logs/events.out.tfevents.1738936984.jupyter-simben-40kth-2ese.423421.0 (deflated 9%)\n",
      "  adding: nnUNetBundle/configs/ (stored 0%)\n",
      "  adding: nnUNetBundle/configs/train_resume.yaml (deflated 60%)\n",
      "  adding: nnUNetBundle/configs/train.yaml (deflated 73%)\n",
      "  adding: nnUNetBundle/configs/metadata.json (deflated 49%)\n",
      "  adding: nnUNetBundle/configs/inference.yaml (deflated 62%)\n",
      "  adding: nnUNetBundle/src/ (stored 0%)\n",
      "  adding: nnUNetBundle/src/__init__.py (stored 0%)\n",
      "  adding: nnUNetBundle/src/mlflow.py (deflated 68%)\n",
      "  adding: nnUNetBundle/src/dataset.py (deflated 43%)\n",
      "  adding: nnUNetBundle/src/trainer.py (deflated 69%)\n",
      "  adding: nnUNetBundle/src/__pycache__/ (stored 0%)\n",
      "  adding: nnUNetBundle/src/__pycache__/__init__.cpython-310.pyc (deflated 22%)\n",
      "  adding: nnUNetBundle/src/__pycache__/trainer.cpython-312.pyc (deflated 53%)\n",
      "  adding: nnUNetBundle/src/__pycache__/mlflow.cpython-312.pyc (deflated 48%)\n",
      "  adding: nnUNetBundle/src/__pycache__/mlflow.cpython-310.pyc (deflated 49%)\n",
      "  adding: nnUNetBundle/src/__pycache__/__init__.cpython-312.pyc (deflated 19%)\n",
      "  adding: nnUNetBundle/src/__pycache__/trainer.cpython-310.pyc (deflated 58%)\n",
      "  adding: nnUNetBundle/src/__pycache__/dataset.cpython-310.pyc (deflated 31%)\n",
      "  adding: nnUNetBundle/environment.yml (deflated 63%)\n",
      "  adding: nnUNetBundle/models/ (stored 0%)\n",
      "  adding: nnUNetBundle/models/dataset.json (deflated 35%)\n",
      "  adding: nnUNetBundle/models/model.pt (deflated 7%)\n",
      "  adding: nnUNetBundle/models/best_model.pt (deflated 7%)\n",
      "  adding: nnUNetBundle/models/plans.json (deflated 91%)\n",
      "  adding: nnUNetBundle/models/nnunet_checkpoint.pth (deflated 57%)\n",
      "  adding: nnUNetBundle/requirements.txt (deflated 60%)\n",
      "  adding: nnUNetBundle/test_input/ (stored 0%)\n",
      "  adding: nnUNetBundle/test_input/spleen_1/ (stored 0%)\n",
      "  adding: nnUNetBundle/test_input/spleen_1/spleen_1.nii.gz (deflated 1%)\n",
      "  adding: nnUNetBundle/nnUNet/ (stored 0%)\n",
      "  adding: nnUNetBundle/nnUNet/params.yaml (deflated 40%)\n",
      "  adding: nnUNetBundle/nnUNet/val_metrics.yaml (deflated 54%)\n",
      "  adding: nnUNetBundle/nnUNet/global.yaml (deflated 40%)\n",
      "  adding: nnUNetBundle/nnUNet/validate.yaml (deflated 52%)\n",
      "  adding: nnUNetBundle/nnUNet/train_handlers.yaml (deflated 58%)\n",
      "  adding: nnUNetBundle/nnUNet/evaluator/ (stored 0%)\n",
      "  adding: nnUNetBundle/nnUNet/evaluator/evaluator.yaml (deflated 25%)\n",
      "  adding: nnUNetBundle/nnUNet/run.yaml (deflated 45%)\n",
      "  adding: nnUNetBundle/nnUNet/val_handlers.yaml (deflated 56%)\n",
      "  adding: nnUNetBundle/nnUNet/train.yaml (deflated 56%)\n",
      "  adding: nnUNetBundle/nnUNet/imports.yaml (deflated 52%)\n",
      "  adding: nnUNetBundle/nnUNet/nnunet_trainer.yaml (deflated 60%)\n",
      "  adding: nnUNetBundle/nnUNet/train_metrics.yaml (deflated 55%)\n",
      "  adding: nnUNetBundle/nnUNet/train_postprocessing.yaml (deflated 70%)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "zip -r Task09_Spleen_Bundle.zip nnUNetBundle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d360d4-124e-43de-9eee-bb353374ae68",
   "metadata": {},
   "source": [
    "## MLFlow Model Upload\n",
    "\n",
    "To store the model and be able to deploy it for inference in future use cases, we can upload it to MLFlow. We will use the MLFlow Python API to log the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18898f8-eff4-4916-9b96-d52cf2170951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maia-user/.conda/envs/MONAI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "from monai.bundle import ConfigParser\n",
    "import torch\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types.schema import Schema, TensorSpec\n",
    "\n",
    "sys.path.append(\"nnUNetBundle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe37c42-7ae8-4233-a049-a28b63ae7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_files = [f.path for f in os.scandir(\"nnUNetBundle/configs\") if f.path.endswith(\"inference.yaml\")]\n",
    "\n",
    "config = {}\n",
    "for config_file in config_files:\n",
    "    with open(config_file, 'r') as file:\n",
    "        config.update(yaml.safe_load(file))\n",
    "\n",
    "config[\"bundle_root\"] = \"nnUNetBundle\"\n",
    "\n",
    "parser = ConfigParser(config,globals={\"os\": \"os\",\n",
    "                                      \"pathlib\":\"pathlib\",\n",
    "                                      \"json\":\"json\",\n",
    "                                      \"ignite\":\"ignite\"\n",
    "                                     })\n",
    "\n",
    "parser.parse(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5954a3d2-1023-4434-bf7e-8ac778c158b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "net = parser.get_parsed_content(\"network_def\",instantiate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0d65dd-d938-4594-9a7b-80ae986f585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.network_weights.load_state_dict(torch.load(\"nnUNetBundle/models/model.pt\")['network_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca526ca8-6cf2-41a6-ad9f-15a2e77f55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed432d0a-9684-4b6e-bc0a-ab1f72cf95ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 178.29it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 512.00it/s]\n",
      "2025/02/12 20:55:01 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - importlib-metadata (current: 8.6.1, required: importlib-metadata==8.5.0)\n",
      " - nnunetv2 (current: 2.5.1, required: nnunetv2==2.5.2)\n",
      " - platformdirs (current: 4.3.6, required: platformdirs==3.11.0)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "Successfully registered model 'Task09_Spleen'.\n",
      "2025/02/12 20:55:09 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Task09_Spleen, version 1\n",
      "Created version '1' of model 'Task09_Spleen'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run nnUNet_Bundle_Spleen at: http://127.0.0.1:5000/#/experiments/444387398057374671/runs/01bd0abe19744cc598acb9a56c2e4ae5\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/444387398057374671\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"nnUNet_Bundle_Spleen\")\n",
    "mlflow.end_run()\n",
    "\n",
    "\n",
    "\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        TensorSpec(np.dtype(np.float32), (1, *net.predictor.configuration_manager.patch_size),name=\"ct\")\n",
    "         \n",
    "    ]\n",
    "    \n",
    ")\n",
    "output_schema = Schema([TensorSpec(np.dtype(np.float32), (1, *net.predictor.configuration_manager.patch_size),name=\"Spleen\")])\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "with mlflow.start_run(run_id='01bd0abe19744cc598acb9a56c2e4ae5'):\n",
    "    mlflow.pytorch.log_model(\n",
    "        net,\n",
    "        \"Task09_Spleen\",\n",
    "        signature=signature,\n",
    "        conda_env = \"/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/environment.yml\",\n",
    "        registered_model_name = \"Task09_Spleen\",\n",
    "        extra_files = [\n",
    "            \"/home/maia-user/Documents/GitHub/tutorials/bundle/Task09_Spleen_Bundle.zip\",\n",
    "            \"/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/environment.yml\",\n",
    "            \"/home/maia-user/Documents/GitHub/tutorials/bundle/nnUNetBundle/requirements.txt\"\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MONAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
